{
    "model_name": "meta-llama/Meta-Llama-3-8B",
    "layers": [4, 5, 6, 7, 8],
    "layer_selection": "specified",
    "fact_token": "subject_last",
    
    "lf_rank": 8,
    "le_rank": 16,
    "alpha": 16.0,
    
    "v_num_grad_steps": 25,
    "v_lr": 3e-4,
    "le_lr": 1e-4,
    "v_loss_layer": 31,
    "v_weight_decay": 1e-4,
    "clamp_norm_factor": 0.75,
    "kl_factor": 0.0625,
    
    "lambda_loc": 0.08,
    "lambda_kl": 0.15,
    "lambda_spec": 0.08,
    "lambda_orth": 0.003,
    
    
    "L2": 1e-4,
    
    "rewrite_module_tmp": "model.layers.{}.mlp.down_proj",
    "layer_module_tmp": "model.layers.{}",
    "mlp_module_tmp": "model.layers.{}.mlp",
    "attn_module_tmp": "model.layers.{}.self_attn",
    "ln_f_module": "model.norm",
    "lm_head_module": "lm_head",
    
    
    "ctx_dim": 4096,
    "hyper_hidden_dim": 256,
    
    "batch_size_forget": 4,
    "batch_size_edit": 4,
    "edit_per_forget": 3
}