from typing import Dict, List, Tuple
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer
from util import nethook
from .ADIT_hparams import ADITHyperParams
from . import repr_tools


def compute_z(
    model: AutoModelForCausalLM,
    tok: AutoTokenizer,
    request: Dict,
    hparams: ADITHyperParams,
    layer: int,
    context_templates: List[str],  # ä»ç„¶ä¿ç•™æ¥å£ï¼Œä½†åœ¨ ROME-style ä¸­ä¸å†ä½¿ç”¨
) -> torch.Tensor:
    """
    ROME-style compute_z:
    --------------------------------------
    æå–ç›®æ ‡ object çš„å†…éƒ¨è¡¨å¾ï¼Œä½œä¸ºç¼–è¾‘ç›®æ ‡å‘é‡ zã€‚
    
    ä¸ä¾èµ–æ¨¡æ¿ï¼Œä¸ä¾èµ– subjectï¼Œä¸ä¾èµ– lookup_idxã€‚
    åªä¾èµ– target_new çš„ token åºåˆ—ï¼Œè·å¾—å…¶åœ¨æŒ‡å®šå±‚çš„ MLP è¾“å‡ºè¡¨ç¤ºã€‚
    """

    # 1. è·å– target_new çš„ string
    target_str = request.get("target_new", {}).get("str", "")
    if not target_str:
        hidden = model.config.hidden_size
        return torch.zeros(hidden, device=model.device)

    # 2. Tokenize target object
    enc = tok(target_str, return_tensors="pt", add_special_tokens=False)
    input_ids = enc["input_ids"].to(model.device)
    attn_mask = enc.get("attention_mask", None)
    if attn_mask is not None:
        attn_mask = attn_mask.to(model.device)

    # 3. ç›®æ ‡æ¨¡å—åç§°
    module_name = hparams.rewrite_module_tmp.format(layer)

    # 4. å‰å‘å¹¶æˆªå–ç›®æ ‡å±‚è¾“å‡º
    with nethook.TraceDict(
        model,
        layers=[module_name],
        retain_output=True,
    ) as tr:
        _ = model(input_ids=input_ids, attention_mask=attn_mask)
        raw = tr[module_name].output

    # 5. ç»Ÿä¸€è¾“å‡ºç»´åº¦ï¼šGPT2 çš„ Conv1D å¯èƒ½è¿”å› tuple æˆ– [seq, hidden]
    if isinstance(raw, tuple):
        raw = raw[0]
    if raw.dim() == 2:
        raw = raw.unsqueeze(0)

    # raw: [1, seq_len, hidden]
    _, seq_len, hidden = raw.shape

    # 6. ROME-styleï¼šå– target object æœ€åä¸€ä¸ª token çš„è¡¨ç¤ºä½œä¸º z
    # ï¼ˆè¿™æ˜¯ ROME ç¨³å®šä¸”æ ‡å‡†çš„åšæ³•ï¼‰
    z = raw[0, -1, :].detach()

    return z



def get_module_input_output_at_words(
    model: AutoModelForCausalLM,
    tok: AutoTokenizer,
    layer: int,
    context_templates: List[str],
    words: List[str],
    module_template: str,
    fact_token_strategy: str,
) -> Tuple[torch.Tensor, torch.Tensor]:
    """
    ADITç‰ˆæœ¬ï¼šè·å–æŒ‡å®šå±‚åœ¨å…³é”®tokenä½ç½®çš„è¾“å…¥å’Œè¾“å‡ºè¡¨ç¤º - ä¿®å¤GPT-2 Conv1Då…¼å®¹æ€§
    """
    print(f"ADIT: Getting module input/output at words for layer {layer}")

    # å‡†å¤‡è¾“å…¥æ–‡æœ¬
    input_texts = []
    for context, word in zip(context_templates, words):
        input_texts.append(context.format(word) if "{}" in context else context)

    # Tokenize
    input_tok = tok(
        input_texts,
        return_tensors="pt",
        padding=True,
    )

    # æ‰¾åˆ°æ¯ä¸ªè¾“å…¥çš„å…³é”®tokenä½ç½®
    lookup_indices = []
    for context, word in zip(context_templates, words):
        idx = find_fact_lookup_idx(context, word, tok, fact_token_strategy, verbose=False)
        lookup_indices.append(idx)

    # è·Ÿè¸ªæŒ‡å®šå±‚çš„è¾“å…¥å’Œè¾“å‡º
    with nethook.TraceDict(
        module=model,
        layers=[module_template.format(layer)],
        retain_input=True,
        retain_output=True,
    ) as tr:
        _ = model(**input_tok)
        
        # è·å–è¾“å…¥å’Œè¾“å‡º
        layer_module = tr[module_template.format(layer)]
        
        # å…³é”®ä¿®å¤ï¼šå¤„ç†GPT-2 Conv1Dçš„ç‰¹æ®Šè¾“å…¥è¾“å‡ºæ ¼å¼
        input_repr = layer_module.input
        output_repr = layer_module.output
        
        # ç»Ÿä¸€å¤„ç†ï¼šå¦‚æœæ˜¯å…ƒç»„ï¼Œå–ç¬¬ä¸€ä¸ªå…ƒç´ 
        if isinstance(input_repr, tuple):
            input_repr = input_repr[0]
        if isinstance(output_repr, tuple):
            output_repr = output_repr[0]
        
        print(f"[DEBUG] Raw input shape: {input_repr.shape}")
        print(f"[DEBUG] Raw output shape: {output_repr.shape}")
        
        # ğŸ”¥ å…³é”®ä¿®å¤ï¼šç¡®ä¿ç»´åº¦æ­£ç¡®ï¼Œå¤„ç†GPT-2å¯èƒ½çš„ç»´åº¦è½¬ç½®
        # å¯¹äºGPT-2 Conv1Då±‚ï¼Œç»´åº¦åº”è¯¥æ˜¯ [batch_size, seq_len, hidden_size]
        if input_repr.dim() == 2:
            # [seq_len, hidden_size] -> [1, seq_len, hidden_size]
            input_repr = input_repr.unsqueeze(0)
        elif input_repr.dim() == 3:
            # æ£€æŸ¥æ˜¯å¦æ˜¯è½¬ç½®çš„ç»´åº¦ [seq_len, batch, hidden]
            if input_repr.shape[0] != len(input_texts):
                # å°è¯•è½¬ç½®åˆ°æ­£ç¡®çš„ç»´åº¦
                if input_repr.shape[1] == len(input_texts):
                    print(f"[DEBUG] Fixing input dimension: transposing {input_repr.shape} -> [{len(input_texts)}, {input_repr.shape[0]}, {input_repr.shape[2]}]")
                    input_repr = input_repr.transpose(0, 1)
        
        if output_repr.dim() == 2:
            output_repr = output_repr.unsqueeze(0)
        elif output_repr.dim() == 3:
            if output_repr.shape[0] != len(input_texts):
                if output_repr.shape[1] == len(input_texts):
                    print(f"[DEBUG] Fixing output dimension: transposing {output_repr.shape} -> [{len(input_texts)}, {output_repr.shape[0]}, {output_repr.shape[2]}]")
                    output_repr = output_repr.transpose(0, 1)

    print(f"[DEBUG] Processed input shape: {input_repr.shape}")
    print(f"[DEBUG] Processed output shape: {output_repr.shape}")

    # æå–å…³é”®ä½ç½®çš„è¾“å…¥å’Œè¾“å‡ºè¡¨ç¤º
    input_vectors = []
    output_vectors = []
    
    batch_size, seq_len, hidden_size = input_repr.shape
    
    for i, idx in enumerate(lookup_indices):
        # ç¡®ä¿ç´¢å¼•åœ¨èŒƒå›´å†…
        if idx >= seq_len:
            idx = seq_len - 1
        elif idx < 0:
            idx = 0
        
        # æå–æŒ‡å®šä½ç½®çš„å‘é‡
        input_vec = input_repr[i, idx, :].detach()
        output_vec = output_repr[i, idx, :].detach()
        
        input_vectors.append(input_vec)
        output_vectors.append(output_vec)

    input_result = torch.stack(input_vectors, dim=0)
    output_result = torch.stack(output_vectors, dim=0)

    print(f"ADIT: Got input shape {input_result.shape}, output shape {output_result.shape}")
    
    return input_result, output_result


def find_fact_lookup_idx(
    prompt: str,
    subject: str,
    tok: AutoTokenizer,
    fact_token_strategy: str,
    verbose: bool = True,
) -> int:
    """
    ADITæŸ¥æ‰¾å…³é”®Tokenä½ç½® â€” æ”¹è¿›ç‰ˆï¼Œè§£å†³tokenizationä¸Šä¸‹æ–‡ä¾èµ–é—®é¢˜
    """
    
    '''if verbose:
        print("\n[DEBUG] find_fact_lookup_idx")
        print("raw: ",prompt)
        print(f"  prompt: {repr(prompt)}")
        print(f"  subject: {repr(subject)}")
        print(f"  strategy: {fact_token_strategy}")'''

    # ç›´æ¥ä½¿ç”¨æˆ‘ä»¬æ”¹è¿›çš„repr_toolså‡½æ•°
    if fact_token_strategy == "last":
        # æœ€åä¸€ä¸ªtokenç­–ç•¥
        result = repr_tools.get_words_idxs_in_templates(
            tok=tok,
            context_templates=[prompt],
            words=[""],  # ç©ºå­—ç¬¦ä¸²ä¼šè§¦å‘é»˜è®¤çš„æœ€åä¸€ä¸ªtokené€»è¾‘
            subtoken="last",  # æ˜ç¡®æŒ‡å®šç­–ç•¥
        )[0][0]
        
    elif fact_token_strategy.startswith("subject_"):
        # subjectç›¸å…³ç­–ç•¥
        subtoken = fact_token_strategy[len("subject_"):]
        
        result = repr_tools.get_words_idxs_in_templates(
            tok=tok,
            context_templates=[prompt],
            words=[subject],
            subtoken=subtoken,
        )[0][0]
        
    else:
        raise ValueError(f"fact_token={fact_token_strategy} not recognized")

    # éªŒè¯å’Œè¾“å‡ºç»“æœ
    if verbose:
        # æ„å»ºå®Œæ•´æ–‡æœ¬ç”¨äºéªŒè¯
        if "{}" in prompt:
            full_text = prompt.format(subject)
        else:
            full_text = prompt + " " + subject if subject else prompt
        
        try:
            # è·å–tokenizationç»“æœ
            encoding = tok(
                full_text,
                return_offsets_mapping=True,
                add_special_tokens=False
            )
            tokens = encoding["input_ids"]
            
            '''if 0 <= result < len(tokens):
                token_at_pos = tok.decode([tokens[result]])
                print(f"  â†’ æœ€ç»ˆä½ç½®: {result}, å¯¹åº”token: '{token_at_pos}'")
            else:
                print(f"  â†’ æœ€ç»ˆä½ç½®: {result} (è¶…å‡ºèŒƒå›´, tokensé•¿åº¦: {len(tokens)})")'''
                
        except:
            # å¦‚æœoffset_mappingå¤±è´¥ï¼Œä½¿ç”¨ç®€å•æ–¹æ³•
            tokens = tok.encode(full_text, add_special_tokens=False)
            '''if 0 <= result < len(tokens):
                token_at_pos = tok.decode([tokens[result]])
                print(f"  â†’ æœ€ç»ˆä½ç½®: {result}, å¯¹åº”token: '{token_at_pos}'")
            else:
                print(f"  â†’ æœ€ç»ˆä½ç½®: {result} (è¶…å‡ºèŒƒå›´)")'''
    
    return result