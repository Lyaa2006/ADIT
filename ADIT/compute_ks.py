from typing import Dict, List
import torch
import numpy as np
from transformers import AutoModelForCausalLM, AutoTokenizer
from .ADIT_hparams import ADITHyperParams
from .compute_z import get_module_input_output_at_words


def compute_ks(
    model: AutoModelForCausalLM,
    tok: AutoTokenizer,
    requests: List[Dict],
    hparams: ADITHyperParams,
    layer: int,
    context_templates: List[str],  # æ³¨æ„ï¼šè¿™é‡Œæ˜¯å­—ç¬¦ä¸²åˆ—è¡¨ï¼Œä¸æ˜¯åµŒå¥—åˆ—è¡¨ï¼
) -> torch.Tensor:
    """
    ADITç‰ˆæœ¬ï¼šè®¡ç®—é”®å‘é‡ - ä¿®å¤æ¨¡æ¿å¤„ç†é—®é¢˜
    """
    print(f"ADIT: Computing key vectors for layer {layer}")
    
    # ğŸ”¥ è°ƒè¯•ï¼šæŸ¥çœ‹ä¼ å…¥çš„æ¨¡æ¿
    print(f"  [DEBUG] Received {len(context_templates)} templates")
    for i, template in enumerate(context_templates[:3]):  # åªæ˜¾ç¤ºå‰3ä¸ª
        print(f"    Template[{i}]: {repr(template)}")
    if len(context_templates) > 3:
        print(f"    ... and {len(context_templates)-3} more")

    # è®¡ç®—æ€»å¤„ç†é‡
    total_contexts = len(context_templates)
    total_processing = len(requests) * total_contexts
    print(f"  Total contexts: {total_contexts}, requests: {len(requests)}")

    # æ„å»ºä¸Šä¸‹æ–‡æ¨¡æ¿å’Œè¯è¯­åˆ—è¡¨
    context_list = []
    words_list = []
    
    for req_idx, request in enumerate(requests):
        subject = request.get("subject", "")
        if not subject:
            print(f"  [WARN] Request {req_idx} has no subject, skipping")
            continue
            
        # ğŸ”¥ ä¿®å¤ï¼šç›´æ¥éå†æ¨¡æ¿åˆ—è¡¨ï¼Œä¸è¦åµŒå¥—å¾ªç¯ï¼
        for template_idx, template in enumerate(context_templates):
            # ç¡®ä¿templateæ˜¯å­—ç¬¦ä¸²
            if not isinstance(template, str):
                print(f"  [ERROR] Template {template_idx} is not string: {type(template)}")
                continue
                
            try:
                # ç”¨ subject æ›¿æ¢æ¨¡æ¿ä¸­çš„ {}
                formatted_context = template.format(subject)
                context_list.append(formatted_context)
                words_list.append(subject)
                
                # è°ƒè¯•è¾“å‡ºï¼ˆåªæ˜¾ç¤ºç¬¬ä¸€ä¸ªæ ·æœ¬çš„ç¬¬ä¸€ä¸ªæ¨¡æ¿ï¼‰
                if req_idx == 0 and template_idx == 0:
                    print(f"  [DEBUG] First template formatting:")
                    print(f"    Raw template: {repr(template)}")
                    print(f"    Subject: {repr(subject)}")
                    print(f"    Formatted: {repr(formatted_context)}")
                    
            except Exception as e:
                print(f"  [ERROR] Failed to format template {template_idx}:")
                print(f"    Template: {repr(template)}")
                print(f"    Subject: {repr(subject)}")
                print(f"    Error: {e}")
                # å¦‚æœæ ¼å¼åŒ–å¤±è´¥ï¼Œç›´æ¥ä½¿ç”¨åŸå§‹æ¨¡æ¿ï¼ˆä¸å¸¦subjectï¼‰
                context_list.append(template)
                words_list.append(subject)

    if not context_list:
        print("  [ERROR] No valid contexts generated!")
        # è¿”å›é›¶å‘é‡
        hidden_size = model.config.hidden_size
        return torch.zeros(len(requests), hidden_size, device=model.device)

    print(f"  Generated {len(context_list)} context strings")
    if context_list:
        print(f"  Sample context: {repr(context_list[0][:50])}...")

    # ä½¿ç”¨ç»Ÿä¸€çš„å‡½æ•°è·å–é”®å‘é‡
    print(f"  [è¿›åº¦] è°ƒç”¨ get_module_input_output_at_words...")
    try:
        input_vectors, output_vectors = get_module_input_output_at_words(
            model=model,
            tok=tok,
            layer=layer,
            context_templates=context_list,
            words=words_list,
            module_template=hparams.rewrite_module_tmp,
            fact_token_strategy=hparams.fact_token,
        )
        
        print(f"  [è¿›åº¦] è·å–åˆ°è¾“å…¥å‘é‡å½¢çŠ¶: {input_vectors.shape}")
        print(f"  [è¿›åº¦] è·å–åˆ°è¾“å‡ºå‘é‡å½¢çŠ¶: {output_vectors.shape}")
        
        # ä½¿ç”¨è¾“å‡ºå‘é‡ä½œä¸ºé”®å‘é‡ï¼ˆMLPå±‚çš„è¾“å‡ºï¼‰
        layer_ks = output_vectors
        print(f"  [è¿›åº¦] ä½¿ç”¨è¾“å‡ºå‘é‡ä½œä¸ºé”®å‘é‡: {layer_ks.shape}")

    except Exception as e:
        print(f"  [ERROR] Failed in get_module_input_output_at_words: {e}")
        # è¿”å›é›¶å‘é‡
        hidden_size = model.config.hidden_size
        return torch.zeros(len(requests), hidden_size, device=model.device)

    # å¹³å‡å¤„ç†
    # ğŸ”¥ æ³¨æ„ï¼šç°åœ¨åªæœ‰ä¸€ä¸ªæ¨¡æ¿ç»„ï¼Œæ‰€ä»¥ç›´æ¥å¹³å‡
    final_keys = []
    
    for i in range(0, layer_ks.size(0), len(context_templates)):
        request_idx = i // len(context_templates)
        if request_idx < len(requests):
            # è·å–è¯¥è¯·æ±‚çš„æ‰€æœ‰æ¨¡æ¿å‘é‡
            template_vectors = layer_ks[i:i+len(context_templates)]
            # å¹³å‡æ‰€æœ‰æ¨¡æ¿
            request_avg = template_vectors.mean(0)
            final_keys.append(request_avg)

    if final_keys:
        result = torch.stack(final_keys, dim=0)
        print(f"ADIT: è®¡ç®—å®Œæˆï¼å¾—åˆ° {result.shape[0]} ä¸ªé”®å‘é‡ï¼Œç»´åº¦: {result.shape}")
    else:
        print("  [ERROR] No final keys generated!")
        hidden_size = model.config.hidden_size
        result = torch.zeros(len(requests), hidden_size, device=model.device)
    
    return result